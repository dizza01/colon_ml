#!/usr/bin/env python3
"""
Demo script for Validation Comparison Features
==============================================

This script demonstrates the new validation dataset comparison functionality
that has been added to the Streamlit app.

Usage: python demo_validation.py
"""

import numpy as np
import matplotlib.pyplot as plt
import torch
from utils import (
    UNet, load_validation_data, sample_validation_images,
    create_comparison_visualization, predict_segmentation,
    calculate_validation_metrics
)

def demo_validation_comparison():
    """Demonstrate the validation comparison functionality"""
    print("ğŸ”¬ Demo: Validation Dataset Comparison")
    print("="*50)
    
    # Load model
    print("ğŸ“¦ Loading model...")
    device = torch.device('cpu')  # Use CPU for demo
    model = UNet(n_class=1)
    
    try:
        checkpoint_path = "data/CVC-ClinicDB/checkpoints/best_model_dice_0.7879_epoch_49.pth"
        checkpoint = torch.load(checkpoint_path, map_location=device)
        model.load_state_dict(checkpoint)
        model.to(device)
        model.eval()
        print("âœ… Model loaded successfully")
    except Exception as e:
        print(f"âš ï¸  Could not load model checkpoint: {e}")
        print("Using randomly initialized model for demo")
    
    # Load validation data
    print("\nğŸ“Š Loading validation data...")
    X_val, y_val, data_available = load_validation_data()
    
    if not data_available:
        print("âŒ Validation data not found.")
        print("Please run the preprocessing section in the notebook first.")
        return
    
    print(f"âœ… Loaded {len(X_val)} validation samples")
    print(f"   Image shape: {X_val.shape}")
    print(f"   Mask shape: {y_val.shape}")
    
    # Sample some images for comparison
    print("\nğŸ² Sampling random validation images...")
    sampled_images, sampled_masks, sample_indices = sample_validation_images(
        X_val, y_val, n_samples=3, random_seed=42
    )
    print(f"Selected samples: {sample_indices}")
    
    # Generate predictions and create comparisons
    print("\nğŸ” Generating predictions and comparisons...")
    
    all_predictions = []
    all_ground_truths = []
    
    for i, idx in enumerate(sample_indices):
        print(f"\nProcessing sample {idx}...")
        
        # Get image and ground truth
        image_tensor = sampled_images[i].unsqueeze(0).to(device)
        ground_truth = sampled_masks[i].squeeze(0).cpu().numpy()
        
        # Generate prediction
        with torch.no_grad():
            prediction, binary_mask = predict_segmentation(model, image_tensor, device)
            prediction_prob = prediction.squeeze().cpu().numpy()
            prediction_mask = binary_mask.squeeze().cpu().numpy()
        
        # Store for batch metrics
        all_predictions.append(prediction_prob)
        all_ground_truths.append(ground_truth)
        
        # Convert image for visualization
        image_np = image_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()
        
        # Create and save comparison visualization
        fig = create_comparison_visualization(
            image_np, ground_truth, prediction_mask, prediction_prob
        )
        
        # Save the figure
        output_path = f"validation_comparison_sample_{idx}.png"
        fig.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close(fig)
        
        print(f"   ğŸ’¾ Saved comparison to: {output_path}")
        
        # Calculate individual metrics
        from utils import calculate_metrics
        metrics = calculate_metrics(
            prediction, 
            torch.tensor(ground_truth).unsqueeze(0).unsqueeze(0)
        )
        
        print(f"   ğŸ“Š Dice Score: {metrics['dice_score']:.3f}")
        print(f"   ğŸ”— IoU: {metrics['iou']:.3f}")
        print(f"   ğŸ“ˆ Pixel Accuracy: {metrics['pixel_accuracy']:.3f}")
    
    # Calculate batch metrics
    print("\nğŸ“ˆ Calculating batch performance metrics...")
    batch_metrics = calculate_validation_metrics(
        np.array(all_predictions), 
        np.array(all_ground_truths)
    )
    
    print(f"   ğŸ¯ Mean Dice Score: {batch_metrics['mean_dice']:.3f} Â± {batch_metrics['std_dice']:.3f}")
    print(f"   ğŸ”— Mean IoU: {batch_metrics['mean_iou']:.3f} Â± {batch_metrics['std_iou']:.3f}")
    print(f"   ğŸ“Š Mean Accuracy: {batch_metrics['mean_accuracy']:.3f} Â± {batch_metrics['std_accuracy']:.3f}")
    
    print("\nğŸ‰ Demo completed successfully!")
    print("\nGenerated files:")
    for idx in sample_indices:
        print(f"   ğŸ“ validation_comparison_sample_{idx}.png")
    
    print(f"\nğŸ’¡ To explore more features, run the Streamlit app:")
    print(f"   streamlit run app.py")
    print(f"   Navigate to: ğŸ”¬ Validation Comparison")

def demo_features_overview():
    """Show what features are available in the validation comparison"""
    print("\nğŸŒŸ Validation Comparison Features Overview")
    print("="*50)
    
    print("ğŸ¯ Individual Sample Analysis:")
    print("   â€¢ Side-by-side comparison: Original â†’ Ground Truth â†’ Prediction")
    print("   â€¢ Prediction probabilities visualization")
    print("   â€¢ Per-sample metrics (Dice, IoU, Pixel Accuracy)")
    print("   â€¢ Random sampling or specific index selection")
    
    print("\nğŸ“Š Batch Performance Analysis:")
    print("   â€¢ Performance metrics over multiple samples")
    print("   â€¢ Statistical summary (mean Â± std)")
    print("   â€¢ Distribution histograms for all metrics")
    print("   â€¢ Configurable sample size")
    
    print("\nğŸ§  Explainability Comparison:")
    print("   â€¢ Explanations overlaid with ground truth")
    print("   â€¢ Attribution quality analysis")
    print("   â€¢ Polyp recall and attribution precision")
    print("   â€¢ Method comparison metrics")
    
    print("\nğŸ“± Interactive Features (in Streamlit app):")
    print("   â€¢ Real-time sample selection")
    print("   â€¢ Interactive metric displays")
    print("   â€¢ Downloadable visualizations")
    print("   â€¢ Responsive layout with error handling")

if __name__ == "__main__":
    try:
        demo_features_overview()
        demo_validation_comparison()
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  Demo interrupted by user")
    except Exception as e:
        print(f"\nâŒ Demo failed with error: {e}")
        print("Check that all dependencies are installed and validation data is available.")
